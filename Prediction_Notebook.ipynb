{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "# import torch.backends.cudnn as cudnn\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "from ad_invento.common.config import Config\n",
    "from ad_invento.common.dist_utils import get_rank\n",
    "from ad_invento.common.registry import registry\n",
    "from ad_invento.conversation.conversation import Chat, CONV_VISION, StoppingCriteriaSub\n",
    "\n",
    "# imports modules for registration\n",
    "from ad_invento.datasets.builders import *\n",
    "from ad_invento.models import *\n",
    "from ad_invento.processors import *\n",
    "from ad_invento.runners import *\n",
    "from ad_invento.tasks import *\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n",
    "from transformers import StoppingCriteriaList\n",
    "\n",
    "# import dataclasses\n",
    "# from enum import auto, Enum\n",
    "# from typing import List, Tuple, Any\n",
    "\n",
    "\n",
    "def setup_seeds(config):\n",
    "    seed = config.run_cfg.seed + get_rank()\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "def create_custom_args(cfg_path, gpu_id, options=None):\n",
    "    args = argparse.Namespace(cfg_path=cfg_path, gpu_id=gpu_id, options=options)\n",
    "    return args\n",
    "\n",
    "# Example values for cfg_path and gpu_id\n",
    "custom_cfg_path = \"eval_configs/invento_eval.yaml\"\n",
    "custom_gpu_id = 0\n",
    "\n",
    "\n",
    "args = create_custom_args(custom_cfg_path, custom_gpu_id)\n",
    "cfg = Config(args)\n",
    "\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = args.gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id)) # 'cuda:{}'.format(args.gpu_id)\n",
    "\n",
    "vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "\n",
    "\n",
    "for name, param in model.visual_encoder.named_parameters():\n",
    "    param.requires_grad=False\n",
    "    \n",
    "for name, param in model.visual_encoder_proj.named_parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "for name, param in model.Qformer.named_parameters():\n",
    "    param.requires_grad=False\n",
    "    \n",
    "for name, param in model.llama_model.named_parameters():\n",
    "    param.requires_grad=False\n",
    "    \n",
    "for name, param in model.llama_proj.named_parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "model.visual_encoder.eval()\n",
    "model.visual_encoder_proj.eval()\n",
    "model.Qformer.eval()\n",
    "model.llama_model.eval()\n",
    "model.llama_proj.eval()\n",
    "\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "gc.collect(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "chat = Chat(model, vis_processor, device='cuda:{}'.format(args.gpu_id)) # device='cuda:{}'.format(args.gpu_id)\n",
    "from PIL import Image\n",
    "\n",
    "img_path = 'F:\\\\Data_kaggle\\\\images\\\\images_normalized\\\\1013_IM-0013-1001.dcm.png'\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "def gradio_reset(chat_state, img_list):\n",
    "    if chat_state is not None:\n",
    "        chat_state.messages = []\n",
    "    if img_list is not None:\n",
    "        img_list = []\n",
    "    return None, gr.update(value=None, interactive=True), gr.update(placeholder='Please upload your image first', interactive=False),gr.update(value=\"Upload & Start Chat\", interactive=True), chat_state, img_list\n",
    "\n",
    "def upload_img(gr_img, text_input, chat_state):\n",
    "    if gr_img is None:\n",
    "        return None, None, gr.update(interactive=True), chat_state, None\n",
    "    chat_state = CONV_VISION.copy()\n",
    "    img_list = []\n",
    "    llm_message = chat.upload_img(gr_img, chat_state, img_list)\n",
    "    return gr.update(interactive=False), gr.update(interactive=True, placeholder='Type and press Enter'), gr.update(value=\"Start Chatting\", interactive=False), chat_state, img_list, llm_message\n",
    "\n",
    "def gradio_ask(user_message, chatbot, chat_state):\n",
    "    if len(user_message) == 0:\n",
    "        return gr.update(interactive=True, placeholder='Input should not be empty!'), chatbot, chat_state\n",
    "    chat.ask(user_message, chat_state)\n",
    "    chatbot = chatbot + [[user_message, None]]\n",
    "    return '', chatbot, chat_state\n",
    "\n",
    "\n",
    "def gradio_answer(chatbot, chat_state, img_list, num_beams, temperature):\n",
    "    llm_message = chat.answer(conv=chat_state,\n",
    "                              img_list=img_list,\n",
    "                              num_beams=num_beams,\n",
    "                              temperature=temperature,\n",
    "                              max_new_tokens=300,\n",
    "                              max_length=64)[0]\n",
    "    chatbot[-1][1] = llm_message\n",
    "    return chatbot, chat_state, img_list\n",
    "\n",
    "x2 = upload_img(gr_img=image, text_input=\"Did you find any abnormality ?\", chat_state=0 )\n",
    "embs = x2[4][0]\n",
    "\n",
    "vis_processor = vis_processor\n",
    "device = 'cuda'\n",
    "stop_words_ids = [torch.tensor([835]).to(device),\n",
    "                  torch.tensor([2277, 29937]).to(device)]  # '###' can be encoded in two different ways.\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "gc.collect(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "outputs = model.llama_model.generate(\n",
    "            inputs_embeds=embs,\n",
    "#             max_new_tokens=300,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            num_beams=5,\n",
    "            num_return_sequences=4,\n",
    "            do_sample=True,\n",
    "            min_length=20,\n",
    "            max_length=64,\n",
    "            top_p=0.8,\n",
    "            top_k=5000,\n",
    "            repetition_penalty=2.0,\n",
    "            length_penalty=1.0,\n",
    "            temperature=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "        )\n",
    "\n",
    "output_token = outputs[0]\n",
    "if output_token[0] == 0:  # the model might output a unknow token <unk> at the beginning. remove it\n",
    "    output_token = output_token[1:]\n",
    "if output_token[0] == 1:  # some users find that there is a start token <s> at the beginning. remove it\n",
    "    output_token = output_token[1:]\n",
    "output_text = model.llama_tokenizer.decode(output_token, add_special_tokens=False)\n",
    "output_text = output_text.split('###')[0]  # remove the stop sign '###'\n",
    "output_text = output_text.split('Assistant:')[-1].strip()\n",
    "\n",
    "print(\"Output Text -> \", output_text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
